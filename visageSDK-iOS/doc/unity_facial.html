<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Introduction</title>
<link href="css/main.css" rel="stylesheet" type="text/css" />
</head>

<body>
<h1>Facial Animation Unity Demo sample project</h1>

<p>
The FacialAnimationUnityDemo sample project demonstrates the integration of visage|SDK with Unity game engine and is aimed at developers starting to use face tracking functionality of visage|SDK to drive facial animation using blendshapes and bones in the Unity game engine.
</p>
<p>
The sample project shows how to animate a character and make it mimic the users facial expressions using action units from visage|SDK. Developers may use parts of this project to speed up their own
development in Unity using visage|SDK or use this application as a starting point for own projects.
</p>

<h2>Building and running the project</h2>

<p>
The files related to FacialAnimationUnityDemo sample project are located in the Samples/iOS/FacialAnimationUnityDemo
subfolder of the visage|SDK for iOS folder.<br/><br/>
Prerequisites to running the sample project are:
<ul>
<li>Xcode (version 7.x)</li>
<li>Unity (version 5.0.x)</li>
<li>iOS device (iPhone 6 or iPad 4 recommended; iPhone 4 and iPhone 4s work with reduced performance)</li>
</ul>

To build and run FacialAnimationUnityDemo sample two steps are involved: creating the Unity project from the provided
Unity package and generating and modifying the Xcode project from Unity. These tasks are executed semi - automatically.<br/><br/>

<b>NOTE:</b> The Visage Tracker Unity plugin for iOS will not work in Unity editor, it will only work on actual device.

</p>

<h3>Step 1. Creating the Unity project from Unity package</h3>

<p>
To create the Unity project from provided Unity package follow these steps:<br/>
<ol>
<li>Open Unity</li> <br/>
<li>Create a new Unity project</li> <br/>
<li>Import <b> FacialAnimationUnityDemo.unitypackage </b>  that is provided in the Samples/iOS/FacialAnimationUnityDemo folder. <br/> 
	This is done in Unity by selecting Assets->Import Package->Custom Package... item from the menu. <br/> 
	All the assets that are used by the example project will be imported into the new project.</li> <br/>
<li>Select and open the Main scene (Jones), located in the Project's Assets/Visage Tracker/Jones/.</li>
</ol>
</p>

<h3>Step 2. Generating the Xcode project from Unity</h3>
<p>
To generate the Xcode project for this sample follow these steps:<br/>
<ol>
<li>In Unity, select <b> File->Build settings...</b> A new window will appear. Switch the platform to iOS and add the current scene.</li> <br/>
<li>Open the player settings by pressing <b>Player Settings...</b> button.</li> <br/>
<li>In <b>PlayerSettings</b> please set the Company name to "visagetechnologies", Product name to "facialanimationunitydemo" and Bundle Identifier
to "com.visagetechnologies.facialanimationunitydemo". Change the <b>Graphics API </b> from <b>Automatic</b> to <b>Open GL ES 2.0.</b></li> <br/>
<li>Press the Build button to generate the project, a new dialog window will appear. In this window, locate and set
destination folder to <b>Samples/iOS/FacialAnimationUnityDemo</b>. This generates Xcode project in that folder.</li><br/>

 <b>NOTE:</b>  If Unity reports error during build, please double check which platform is selected in the Build settings. <br/>  <br/> 

<li>Contrary to VisageTrackerUnity sample, where a custom post build script is used, this sample deomonstrates the use of Unity mechanisms for including visage|SDK data files. All the necessary data will be copied from Unity's StreamingAssets folder to the \Data\Raw\ folder of the Xcode project. Note that copied files will not be visible in the generated Xcode project.<br>Libraries however must be added manually as instructed: copy <b>libVisageVision.a</b>, <b>libVisageCore.a</b> and <b>libVisageTrackerUnityPlugin.a</b> from visage|SDK lib folder to the Libraries folder of the generated Xcode project.
After that just drag and drop them from that folder to the Xcode project (also to the Libraries folder).</li><br/>
<li>Please double check that targeted iOS device is selected as active scheme. Go to Build Settings, set iOS Deploymenet Target to 8.1 and Enable Bitcode to NO.  <br/>
Build the Xcode project. <br/>
</li><br/>
<b>NOTE:</b>  Sometimes Xcode returns link error that it can't locate libiPhone-lib.a. In such cases click on Project->Build Phases->Link Binary With Libraries, remove the libiPhone-lib.a and add it again. To add a new library click + option, a new window will appear, select Add Other...->Libraries->libiPhone-lib.a. <br/> <br/>
<li>This step is optional. If you have a license key file, add it by dragging and dropping them into newly created Resources
group in generated Xcode project. Also, please read the section <a href="licensing.html#include_license">Including the License Key File in your application</a> in the Registration and Licensing page.</li><br/>
<b>NOTE:</b>  In case you receive error message that App Transport Security has blocked a cleartext HTTP (http://) resource load, please set the NSAllowsArbitraryLoads key to YES under NSAppTransportSecurity dictionary in your .plist file <br/> <br/>
</ol>
</p>

</p>
<p>
This sample is designed to be cross-platform for Windows, iOS and Android. If you want to add support for Windows and iOS import the sample source code from visage|SDK for Windows and visage|SDK for iOS into the same Unity project and follow instructions for each platform.
</p>

<h2>Using the sample app</h2>
<p>
As soon as the application starts, tracking will begin. When a face is found in the camera frame, the character will
start moving - mimicking the user's facial expressions. Try opening your mouth, smiling or moving your eyebrows.
</p>
<p>
	<b> NOTE:</b>  Without the valid License Key tracking will stop after 1 minute. Appropriate message will appear on the screen to inform the user before the start of the tracking and after the tracking time expires. 
</p>

<h3>Implementation overview</h3>
<p>
The project consists of a main scene with the Unity’s GameObjects that provide different functionalities. A GameObject
is the main building block of a scene in Unity. It consists of different Components and can parent other GameObjects.
Manipulation of GameObjects is done with Scripts. It is recommended for developers to get familiar with Unity basics
before continuing. The integration with visage|SDK is done through the VisageTrackerUnityPlugin.
</p>

<h4>Visage Tracker GameObject</h4>
<p>
The core of the example is the Visage Tracker GameObject. It handles communication with the tracker
(starts it and gets results from it). The behaviour of the script can be modified by changing its properties and the script
code. Other properties can be added as well. The existing Visage Tracker script properties of interest are as follows:<br/>
• Config File iOS: filename of the tracker configuration file (see the <a href="VisageTracker Configuration Manual.pdf">VisageTracker Configuration manual</a> for information on modifying the tracker configuration) that will be used by the tracker for Android<br/>
• BindingConfigurations: list of binding configurations (see the Binding Configuration section on this page) used in the scene<br/>
</p>

<h4>Cameras</h4>
<p>
There are two cameras in the scene, one for displaying the character (Main Camera) and other for the background (Background Camera).
The main camera "Field of View" is automatically set according to the Visage Tracker focus (camera_focus parameter in
configuration file).
</p>

<h4>Jones</h4>
<p>
Jones is the character with defined blendshape animations which are controlled using the a binding configuration and bones
which are controlled using the <i>Visage Tracker/Jones/Scripts/Movement.cs</i> script. The visage|SDK provides relevant information about facial expressions through action units. Their values are then mapped to specific blendshapes using 
binding configurations in the <i>VisageTracker.cs</i> script. For more information on creating characters with such blendshapes, consult the <a href="modeling_guide.pdf" target="_blank">Modeling Guide</a>.
</p>

<h4>Binding Configuration</h4>
<p>
Binding configurations map action unit values to specific blendshapes and animate them. A reference binding configuration used to animate Jones is located in <i>Visage Tracker/Jones/jones.bind.txt</i>. <br/>
<i>ActionUnitBinding.cs</i> components are created based on the <i>VisageTracker.cs</i> Binding Configuration list in the current scene at runtime. They receive action unit data from the Visage Tracker component and map them to [0,1] values based on the info given in the binding configuration. You can omit the binding configuration in the Visage Tracker GameObject and create the <i>ActionUnitBinding.cs</i> components manually for more control.
</p>

<h4>Visage Tracker Unity Plugin</h4>
<p>
The integration of visage|SDK face tracking with Unity is done through a plugin that wraps native code calls and
provides functions that can be called from Unity scripts. For more information about the plugin including it's source code, see the <a href="unity_plugin.html">VisageTrackerUnityPlugin</a> project and its documentation.
</p>

</body>
</html>
